import requests
import json

# CThis line constructs the URL for the USGS Earthquake API. .
# It uses start_date and end_date to specify the time range for the earthquake data.
url = f"https://earthquake.usgs.gov/fdsnws/event/1/query?format=geojson&starttime={start_date}&endtime={end_date}"

# Make the GET request to fetch data
response = requests.get(url)

# This checks if the request was successful (HTTP status code 200)
# If the request is successful, the JSON response is parsed and the relevant data (features) is extracted
if response.status_code == 200:
    # Get the JSON response
    data = response.json()
    data = data['features']
    
    # This line specifies the file path where the data will be saved. The file name includes the start_date.
    file_path = f'/lakehouse/default/Files/{start_date}_earthquake_data.json'
    
    # Open the file in write mode ('w') and save the JSON data
    with open(file_path, 'w') as file:
        # The `json.dump` method serializes `data` as a JSON formatted stream to `file`
        
        # `indent=4` makes the file human-readable by adding whitespace
        json.dump(data, file, indent=4)
        
    print(f"Data successfully saved to {file_path}")
else:
    print("Failed to fetch data. Status code:", response.status_code)



df = spark.read.option("multiline", "true").json("Files/2024-08-15_earthquake_data.json")
# df now is a Spark DataFrame containing JSON data from "Files/2024-08-15_earthquake_data.json".
display(df)



Silver Notebook

from pyspark.sql.functions import col
from pyspark.sql.types import TimestampType
from datetime import date , timedelta


#extracting file based on date into a dataframe
earthquake_df = spark.read.option("multiline", "true").json(f"Files/{start_date}_earthquake_data.json")


#getting first item of column geometry , which is and langitutde
Longatude = earthquake_df.select(col("geometry.coordinates").getItem(0))

#getting second item of column geometry , which is Latitude
Latitude = earthquake_df.select(col("geometry.coordinates").getItem(1))

#getting third item of column geometry , which is Elevation
Elevation = earthquake_df.select(col("geometry.coordinates").getItem(2))

#getting event type
display(earthquake_df.select(col("properties")))


#Getting magnitude
display(earthquake_df.select(col("properties.mag")))

# Reshape earthquake data by extracting and renaming key attributes for further analysis.
#This step is doing all the previous steps all at one go, extracting parts of the column values.

earthquake_df = \
earthquake_df.\
    select(
        'id',
        col('geometry.coordinates').getItem(0).alias('longitude'),
        col('geometry.coordinates').getItem(1).alias('latitude'),
        col('geometry.coordinates').getItem(2).alias('elevation'),
        col('properties.title').alias('title'),
        col('properties.place').alias('place_description'),
        col('properties.sig').alias('sig'),
        col('properties.mag').alias('mag'),
        col('properties.magType').alias('magType'),
        col('properties.time').alias('time'),
        col('properties.updated').alias('updated')
        )

# Convert 'time' and 'updated' columns from milliseconds to timestamp format for clearer datetime representation.
earthquake_df = earthquake_df.\
    withColumn('time', col('time')/1000).\
    withColumn('updated', col('updated')/1000).\
    withColumn('time', col('time').cast(TimestampType())).\
    withColumn('updated', col('updated').cast(TimestampType()))


# appending the data to the gold table
earthquake_df.write.mode('append').saveAsTable('earthquake_events_silver')



Gold Layers

#reading silver table into gold table. filtred only recent dates with will be determined by ADF
earthquake_df = spark.read.table("earthquake_events_silver").filter(col('time') > start_date)


#finding reseraching the following coordinates
coordinates = (40.9584,83.9772)
rg.search(coordinates)


#finding reseraching the following coordinates
coordinates = (40.9584,83.9772)
rg.search(coordinates)[0].get("cc")


def get_country_code(lat, lon):
    """
    Retrieve the country code for a given latitude and longitude.

    Parameters:
    lat (float or str): Latitude of the location.
    lon (float or str): Longitude of the location.

    Returns:
    str: Country code of the location, retrieved using the reverse geocoding API.

    Example:
    >>> get_country_details(48.8588443, 2.2943506)
    'FR'
    """
    coordinates = (float(lat), float(lon))
    return rg.search(coordinates)[0].get('cc')


# registering the udfs so they can be used on spark dataframes
get_country_code_udf = udf(get_country_code, StringType())


#adds a new column named "country_code"
#entries in the "country_code" column are generated by calling the get_country_code_udf() function, which processes the latitude and longitude of each earthquake.
df_with_location = \
                earthquake_df.\
                    withColumn("country_code", get_country_code_udf(col("latitude"), col("longitude")))


# adding significance classification
# It creates a new DataFrame called df_with_location_sig_class based on the df_with_location DataFrame.
# A new column named 'sig_class' is added to this new DataFrame.
# The values in the 'sig_class' column are determined based on the conditions applied to the existing 'sig' column:
# If "sig" is less than 100, 'sig_class' is set to "Low".
# If "sig" is between 100 and 499, 'sig_class' is set to "Moderate".
# If "sig" is 500 or higher, 'sig_class' is set to "High".

df_with_location_sig_class = \
                            df_with_location.\
                                withColumn('sig_class', 
                                            when(col("sig") < 100, "Low").\
                                            when((col("sig") >= 100) & (col("sig") < 500), "Moderate").\
                                            otherwise("High")
                                            )

# this code appends the data from the df_with_location_sig_class DataFrame to an existing table named earthquake_events_gold in a database.
df_with_location_sig_class.write.mode('append').saveAsTable('earthquake_events_gold')















